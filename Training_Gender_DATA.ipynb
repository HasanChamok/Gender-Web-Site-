{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ELcwOnqu1BIy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec658331-1499-49bd-a011-aa5268ba1527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.203-py3-none-any.whl (644 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.8/644.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.16.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from ultralytics)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: thop, ultralytics\n",
            "Successfully installed thop-0.1.1.post2209072238 ultralytics-8.0.203\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "!pip install tqdm --upgrade\n",
        "\n",
        "from tqdm.notebook import tqdm\n"
      ],
      "metadata": {
        "id": "lCOHs8Op62Gz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d660189c-1f7d-49b7-8a34-689b9e7e7ef3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "J1LKVO0B7jlB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca861e1-2a42-4c07-ed0b-72fbe678d148"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path_img = './MyDrive/YOLOv8_dataset_gender/train/images'\n",
        "train_path_label = './MyDrive/YOLOv8_dataset_gender/train/labels'\n",
        "val_path_img = './MyDrive/YOLOv8_dataset_gender/valid/images'\n",
        "val_path_label = './MyDrive/YOLOv8_dataset_gender/valid/labels'\n",
        "test_path_img = './MyDrive/YOLOv8_dataset_gender/test/images'\n",
        "test_path_label = './MyDrive/YOLOv8_dataset_gender/test/label'"
      ],
      "metadata": {
        "id": "SpFjLfFO7w9X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "id": "0nwMaoMBIWse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77125a95-faa1-4cec-e11f-6c0246e78828"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.203 🚀 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 27.1/78.2 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=train model=yolov8s.pt data=/content/drive/MyDrive/YOLov8_dataset_gender/data.yaml epochs=10 imgsz=640 batch=8 project=/content/drive/MyDrive/YOLov8_dataset_gender/Result name=gender"
      ],
      "metadata": {
        "id": "8rOOEAfTCvRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7120ff9b-c4a3-44a4-cc87-ef980ccb0df9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt to 'yolov8s.pt'...\n",
            "100% 21.5M/21.5M [00:00<00:00, 225MB/s]\n",
            "Ultralytics YOLOv8.0.203 🚀 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/content/drive/MyDrive/YOLov8_dataset_gender/data.yaml, epochs=10, patience=50, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=/content/drive/MyDrive/YOLov8_dataset_gender/Result, name=gender, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/content/drive/MyDrive/YOLov8_dataset_gender/Result/gender\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 20.9MB/s]\n",
            "2023-10-30 06:34:13.196051: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-10-30 06:34:13.196110: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-10-30 06:34:13.196154: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2116822  ultralytics.nn.modules.head.Detect           [2, [128, 256, 512]]          \n",
            "Model summary: 225 layers, 11136374 parameters, 11136358 gradients, 28.6 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/drive/MyDrive/YOLov8_dataset_gender/Result/gender', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "100% 6.23M/6.23M [00:00<00:00, 110MB/s]\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/YOLov8_dataset_gender/train/labels... 5711 images, 0 backgrounds, 0 corrupt: 100% 5711/5711 [27:18<00:00,  3.48it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/YOLov8_dataset_gender/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/YOLov8_dataset_gender/valid/labels... 562 images, 0 backgrounds, 0 corrupt: 100% 562/562 [02:53<00:00,  3.25it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/YOLov8_dataset_gender/valid/labels.cache\n",
            "Plotting labels to /content/drive/MyDrive/YOLov8_dataset_gender/Result/gender/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/YOLov8_dataset_gender/Result/gender\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/10      2.15G      0.778      1.521       1.26          7        640: 100% 714/714 [02:45<00:00,  4.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 36/36 [00:10<00:00,  3.51it/s]\n",
            "                   all        562        773      0.234      0.614       0.32      0.172\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/10      2.23G     0.6901      1.077      1.205          7        640: 100% 714/714 [02:39<00:00,  4.47it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 36/36 [00:07<00:00,  4.67it/s]\n",
            "                   all        562        773      0.636      0.532      0.624      0.468\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/10      2.23G     0.6605     0.9643      1.196         11        640: 100% 714/714 [02:41<00:00,  4.42it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 36/36 [00:07<00:00,  5.04it/s]\n",
            "                   all        562        773      0.645      0.634      0.671      0.498\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/10      2.23G     0.6398     0.8824      1.161         13        640: 100% 714/714 [02:41<00:00,  4.42it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 36/36 [00:07<00:00,  4.79it/s]\n",
            "                   all        562        773      0.675      0.629      0.686      0.494\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/10      2.22G     0.6131     0.8488      1.145         23        640: 100% 714/714 [02:42<00:00,  4.41it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 36/36 [00:09<00:00,  3.82it/s]\n",
            "                   all        562        773      0.704      0.624      0.687      0.504\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/10      2.23G     0.5899     0.7698      1.126         11        640: 100% 714/714 [02:37<00:00,  4.53it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 36/36 [00:08<00:00,  4.03it/s]\n",
            "                   all        562        773      0.725       0.64      0.725      0.532\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/10      2.23G     0.5748     0.7216      1.117          7        640: 100% 714/714 [02:36<00:00,  4.56it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 36/36 [00:09<00:00,  3.98it/s]\n",
            "                   all        562        773      0.722      0.672      0.747      0.556\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/10      2.23G     0.5563     0.6724      1.099          7        640: 100% 714/714 [02:35<00:00,  4.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 36/36 [00:08<00:00,  4.03it/s]\n",
            "                   all        562        773      0.762      0.655      0.756      0.552\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/10      2.23G     0.5391     0.6118      1.082         15        640: 100% 714/714 [02:38<00:00,  4.51it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 36/36 [00:09<00:00,  3.98it/s]\n",
            "                   all        562        773      0.697       0.76      0.788       0.58\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/10      2.23G     0.5187     0.5749      1.074          7        640: 100% 714/714 [02:35<00:00,  4.58it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 36/36 [00:08<00:00,  4.01it/s]\n",
            "                   all        562        773      0.773      0.719       0.79      0.589\n",
            "\n",
            "10 epochs completed in 0.472 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/YOLov8_dataset_gender/Result/gender/weights/last.pt, 22.5MB\n",
            "Optimizer stripped from /content/drive/MyDrive/YOLov8_dataset_gender/Result/gender/weights/best.pt, 22.5MB\n",
            "\n",
            "Validating /content/drive/MyDrive/YOLov8_dataset_gender/Result/gender/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.203 🚀 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 36/36 [00:10<00:00,  3.34it/s]\n",
            "                   all        562        773      0.777      0.717      0.789      0.589\n",
            "                female        562        364      0.732      0.684      0.761      0.532\n",
            "                  male        562        409      0.821      0.751      0.818      0.646\n",
            "Speed: 0.4ms preprocess, 4.7ms inference, 0.0ms loss, 2.5ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/YOLov8_dataset_gender/Result/gender\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=predict model=/content/drive/MyDrive/YOLov8_dataset_gender/Result/gender/weights/best.pt conf=0.55 source=/content/drive/MyDrive/YOLov8_dataset_gender/test/images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbOW-5u-otmI",
        "outputId": "432df81b-3ed2-4dba-cd20-88c309094813"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.0.203 🚀 Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 11126358 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\n",
            "image 1/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0825036_SPI_01_0_00_0_01_jpg.rf.4519fe25ba63950d2c65b4ee07b19582.jpg: 640x640 1 male, 16.3ms\n",
            "image 2/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826007_SPI_01_0_00_0_01_jpg.rf.dff4616f3c8f2784d0751dc36d722a8d.jpg: 640x640 1 female, 53.7ms\n",
            "image 3/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826009_SPI_02_1_2D_2_03_jpg.rf.05566dac8d76f7d35cb27f2d5755ddc0.jpg: 640x640 1 female, 16.5ms\n",
            "image 4/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826009_SPI_04_1_2D_1_03_jpg.rf.2621694c9b93e221e5ed500faf29652f.jpg: 640x640 1 female, 16.3ms\n",
            "image 5/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826010_SPI_03_1_2D_2_02_jpg.rf.b526210a7ca5921013931164e7a7bcfb.jpg: 640x640 1 female, 16.2ms\n",
            "image 6/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826010_SPI_04_1_2D_1_03_jpg.rf.604955fa028a180a20e01e5c6f4d1362.jpg: 640x640 1 female, 16.2ms\n",
            "image 7/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826013_SPI_04_1_2D_1_03_jpg.rf.2569efe6813fac15889e6af7550a3164.jpg: 640x640 1 female, 16.2ms\n",
            "image 8/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826013_SPI_07_1_2D_3_02_jpg.rf.5d57dce5b3340cd6edad63098f73176d.jpg: 640x640 1 female, 16.2ms\n",
            "image 9/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826014_SPI_02_1_2D_2_03_jpg.rf.89dcfc7adb33131f22923a2321df68ac.jpg: 640x640 1 male, 16.2ms\n",
            "image 10/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826014_SPI_05_1_2D_1_02_jpg.rf.1bc72b0e8defea4aca677a0f4650ac83.jpg: 640x640 1 male, 14.4ms\n",
            "image 11/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826015_SPI_07_1_2D_3_02_jpg.rf.feaf9c05c5babedda48bdc8bcd0ff187.jpg: 640x640 1 male, 14.3ms\n",
            "image 12/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826020_SPI_05_1_2D_1_02_jpg.rf.42446296f58ad0edd8e794a5dcf22e47.jpg: 640x640 1 female, 14.6ms\n",
            "image 13/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826024_SPI_05_1_2D_1_02_jpg.rf.9f8cc7c7d73745828bdcccd2218ff2e1.jpg: 640x640 1 female, 14.3ms\n",
            "image 14/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826025_SPI_03_1_2D_2_02_jpg.rf.02c9189352af8c2e697f28222cb716a5.jpg: 640x640 1 male, 14.3ms\n",
            "image 15/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826026_SPI_05_1_2D_1_02_jpg.rf.d24e5ab0aa15f369d2a1237267735230.jpg: 640x640 1 female, 14.3ms\n",
            "image 16/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826033_SPI_04_1_2D_1_03_jpg.rf.ccb9392bc6348e1025cab46ad52dda07.jpg: 640x640 1 female, 13.1ms\n",
            "image 17/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826037_SPI_02_1_2D_2_03_jpg.rf.50d6166724a81f8bd1b4c384d0f0f29c.jpg: 640x640 1 female, 13.1ms\n",
            "image 18/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0826037_SPI_07_1_2D_3_02_jpg.rf.d623ba43aab03cf881cb5f11ee6ba07a.jpg: 640x640 1 female, 13.2ms\n",
            "image 19/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0827007_SPI_01_0_00_0_01_jpg.rf.c43c00d068d323bfc03e2f18b02e7941.jpg: 640x640 1 female, 13.1ms\n",
            "image 20/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0827007_SPI_07_1_2D_3_02_jpg.rf.6b9c457481e288567235f90d6f7e56d1.jpg: 640x640 1 female, 13.2ms\n",
            "image 21/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0827010_SPI_06_1_2D_3_03_jpg.rf.8b621002ae3115be52dd472f43980038.jpg: 640x640 (no detections), 13.1ms\n",
            "image 22/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0827017_SPI_06_1_2D_3_03_jpg.rf.57d98383141eca901bb5e132ee6b54eb.jpg: 640x640 1 female, 12.9ms\n",
            "image 23/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0827018_SPI_07_1_2D_3_02_jpg.rf.909daaec9e3ea2d9551e96f765fbba53.jpg: 640x640 1 female, 12.5ms\n",
            "image 24/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0827019_SPI_07_1_2D_3_02_jpg.rf.851aa7b0ab94bb8af576fd3cd1a96f0d.jpg: 640x640 1 female, 1 male, 12.5ms\n",
            "image 25/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0827021_SPI_01_0_00_0_01_jpg.rf.4a69a16549bee4c6366a3498c140148d.jpg: 640x640 1 male, 12.5ms\n",
            "image 26/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0827023_SPI_06_1_2D_3_03_jpg.rf.ade6db22499fafdcbfe925e747343a80.jpg: 640x640 1 male, 12.5ms\n",
            "image 27/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0827030_SPI_03_1_2D_2_02_jpg.rf.44920d4bcddf9cb6074340c5c2d83a50.jpg: 640x640 1 female, 12.5ms\n",
            "image 28/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0827031_SPI_02_1_2D_2_03_jpg.rf.9d35ad4326f3a3fc7a92a50c8bcc9068.jpg: 640x640 1 female, 12.6ms\n",
            "image 29/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0827032_SPI_03_1_2D_2_02_jpg.rf.e319ce92565c0e3801be167f0044eba5.jpg: 640x640 1 male, 11.9ms\n",
            "image 30/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0830001_SPI_02_1_3D_2_03_jpg.rf.c410a956a8bbe847bdf1e03a7c017aa3.jpg: 640x640 1 male, 11.8ms\n",
            "image 31/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0830001_SPI_03_1_3D_2_02_jpg.rf.08dc17471f3c48f2f46bee13d96dda08.jpg: 640x640 1 male, 11.9ms\n",
            "image 32/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0830002_SPI_06_1_3D_3_03_jpg.rf.b9498cb702b0b1308fc1285621984273.jpg: 640x640 1 female, 11.9ms\n",
            "image 33/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0830003_SPI_06_1_3D_3_03_jpg.rf.a7d3b77430c3b49735337ba02f45e619.jpg: 640x640 1 female, 11.8ms\n",
            "image 34/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0830011_SPI_07_1_3D_3_02_jpg.rf.d3c267c335d51e949248a60be4ba16d8.jpg: 640x640 1 female, 11.9ms\n",
            "image 35/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0830016_SPI_03_1_3D_2_02_jpg.rf.99b7782b488a0b02b6cfcb3e46de8b91.jpg: 640x640 1 female, 11.9ms\n",
            "image 36/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0830017_SPI_05_1_3D_1_02_jpg.rf.828d4fac99809734c86ef158d8177be4.jpg: 640x640 1 male, 13.6ms\n",
            "image 37/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0830017_SPI_06_1_3D_3_03_jpg.rf.dd94debf22d392b11ee3a0109c9e4454.jpg: 640x640 1 male, 13.1ms\n",
            "image 38/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0830021_SPI_01_0_00_0_01_jpg.rf.b34e13cce27043d071f9e7cce8d11b81.jpg: 640x640 1 male, 11.9ms\n",
            "image 39/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0830026_SPI_01_0_00_0_01_jpg.rf.532b4a4cbfe0715fbb5655e42cc9470f.jpg: 640x640 1 male, 11.9ms\n",
            "image 40/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0830027_SPI_07_1_3D_3_02_jpg.rf.af6dff9963baae3626bece3c15950a59.jpg: 640x640 1 female, 11.9ms\n",
            "image 41/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0830029_SPI_01_0_00_0_01_jpg.rf.ef0f9283879eb3f1968c443358dbb10a.jpg: 640x640 1 female, 11.9ms\n",
            "image 42/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0830029_SPI_05_1_3D_1_02_jpg.rf.4dc9f212cc27101462b5b57cbd7a0eb0.jpg: 640x640 1 female, 11.9ms\n",
            "image 43/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0830030_SPI_06_1_3D_3_03_jpg.rf.ca37f3bdeae83c34993f5f94979b7ea8.jpg: 640x640 1 female, 12.0ms\n",
            "image 44/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0831001_SPI_02_1_3D_2_03_jpg.rf.f0f4a28ee9de503ac5fbfa456c3a46a5.jpg: 640x640 1 male, 11.9ms\n",
            "image 45/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0831003_SPI_02_1_3D_2_03_jpg.rf.123cfe82bd71eae631a10b726d73b21a.jpg: 640x640 1 male, 11.9ms\n",
            "image 46/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0831005_SPI_01_0_00_0_01_jpg.rf.719f9a06782fdaaf0d83449800a8d29a.jpg: 640x640 1 female, 11.9ms\n",
            "image 47/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0831011_SPI_03_1_3D_2_02_jpg.rf.3ccdb17787f0b3aea07e478a1ef1c386.jpg: 640x640 1 female, 11.9ms\n",
            "image 48/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0831019_SPI_05_1_3D_1_02_jpg.rf.2251054ef1dcd3b06ec8316bbfa8846e.jpg: 640x640 1 female, 13.7ms\n",
            "image 49/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0831027_SPI_02_1_3D_2_03_jpg.rf.4b77b0475b6986f5e82005169c697cd2.jpg: 640x640 1 female, 11.9ms\n",
            "image 50/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0831027_SPI_07_1_3D_3_02_jpg.rf.d1e6299fce4456f353f13991458b4737.jpg: 640x640 1 female, 11.9ms\n",
            "image 51/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0831031_SPI_05_1_3D_1_02_jpg.rf.f75921646650e781c9f9152c32455c93.jpg: 640x640 1 female, 11.9ms\n",
            "image 52/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0831032_SPI_03_1_3D_2_02_jpg.rf.049c2e2ed0ea81b839adf836d0fff34b.jpg: 640x640 1 male, 11.9ms\n",
            "image 53/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0901004_SPI_02_1_3D_2_03_jpg.rf.ccabfb2742206c70d7124bf973a8759a.jpg: 640x640 1 male, 11.9ms\n",
            "image 54/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0901008_SPI_01_0_00_0_01_jpg.rf.c253e3d20e32dadab0a35cebce43897d.jpg: 640x640 1 female, 11.9ms\n",
            "image 55/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0901009_SPI_01_0_00_0_01_jpg.rf.7f941a217bc876481573f8fa19036ef1.jpg: 640x640 1 female, 11.9ms\n",
            "image 56/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0901015_SPI_02_1_3D_2_03_jpg.rf.32912a74426d0fc264af12193ce57c34.jpg: 640x640 1 female, 11.9ms\n",
            "image 57/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0901017_SPI_01_0_00_0_01_jpg.rf.99e5726711984cee851a1f7b72107d7a.jpg: 640x640 1 male, 11.9ms\n",
            "image 58/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/0901021_SPI_02_1_3D_2_03_jpg.rf.4428e59926123893503547fe4b3e70d7.jpg: 640x640 1 female, 11.9ms\n",
            "image 59/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1070_jpg.rf.f3df88afc6c624c20d219407cea70ce7.jpg: 640x640 1 female, 11.9ms\n",
            "image 60/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1079_jpg.rf.779a7153356f0dcadb7873bea48e5c49.jpg: 640x640 1 male, 11.9ms\n",
            "image 61/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1098_jpg.rf.5a9ac4930370ff7933b1a956f04db7c9.jpg: 640x640 1 female, 11.9ms\n",
            "image 62/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1123_jpg.rf.1f5291c1173fb6d5d2f5ad60fba9f7eb.jpg: 640x640 1 male, 11.9ms\n",
            "image 63/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1144_jpg.rf.1128d904022373afd6b14865fc41cd14.jpg: 640x640 1 male, 11.9ms\n",
            "image 64/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1163_jpg.rf.99232ae1a0ccbec45bfe962b0427ade3.jpg: 640x640 1 female, 11.9ms\n",
            "image 65/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1167_jpg.rf.6000853855c9d56f8f0a3dd6561e093e.jpg: 640x640 1 female, 11.9ms\n",
            "image 66/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1199_jpg.rf.c1ddea575d333e26501ddccc3d64314f.jpg: 640x640 1 female, 11.9ms\n",
            "image 67/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1248_jpg.rf.67e8218a0cca95afc97598e6bddc108a.jpg: 640x640 1 female, 11.9ms\n",
            "image 68/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1284_jpg.rf.00b7d3baa7dd408e54e1785dbdef30ab.jpg: 640x640 1 male, 11.9ms\n",
            "image 69/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_12_jpg.rf.077f999efd148d2659c4bc2860bd6cca.jpg: 640x640 1 male, 11.9ms\n",
            "image 70/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1324_jpg.rf.177f17d817063e02c28ab236ad8cc71d.jpg: 640x640 1 male, 11.9ms\n",
            "image 71/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_136_jpg.rf.62f8c7c1028c41385c386605b3ecf169.jpg: 640x640 1 female, 11.9ms\n",
            "image 72/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1381_jpg.rf.f81b1f87bd6841ad3755bf85c7c9a13b.jpg: 640x640 1 male, 11.9ms\n",
            "image 73/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_138_jpg.rf.905f1aa2f19f602cea06849fdc4fcd19.jpg: 640x640 1 male, 11.9ms\n",
            "image 74/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1434_jpg.rf.1cc19120caf76f7151f1bc56ed2bd7f6.jpg: 640x640 1 male, 11.9ms\n",
            "image 75/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1449_jpg.rf.14d09584cce11228ed4e4534115e7eba.jpg: 640x640 1 male, 11.9ms\n",
            "image 76/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1450_jpg.rf.3323a42f05d97cabfce5adc9633cf979.jpg: 640x640 1 male, 13.4ms\n",
            "image 77/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1458_jpg.rf.303d804da347fef7f91000249a75d7b8.jpg: 640x640 1 male, 13.4ms\n",
            "image 78/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1466_jpg.rf.892e3c514cc81afef30ec3ff035f64b8.jpg: 640x640 1 male, 13.4ms\n",
            "image 79/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1487_jpg.rf.9ae14eb8606926863395c5d0d5fd41ff.jpg: 640x640 1 male, 13.4ms\n",
            "image 80/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1492_jpg.rf.87caa1529598f9e1f4a35e89d8b55e86.jpg: 640x640 1 male, 13.4ms\n",
            "image 81/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_1496_jpg.rf.5fdab6cd524fec57e7d944641681c20d.jpg: 640x640 1 male, 13.4ms\n",
            "image 82/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_149_jpg.rf.31a96f50ac5a92397fba3662c73d1fd2.jpg: 640x640 1 male, 13.6ms\n",
            "image 83/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_215_jpg.rf.9a37ac48c5d106a207c81c36a5d3907f.jpg: 640x640 1 female, 13.6ms\n",
            "image 84/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_216_jpg.rf.3a49de66b6c9f4cc343eb60562d9dab3.jpg: 640x640 1 female, 13.7ms\n",
            "image 85/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_217_jpg.rf.4c8c711e59f8327e637ef9e2010d9264.jpg: 640x640 1 male, 13.6ms\n",
            "image 86/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_22_jpg.rf.0bb7abd23a30acdee926fb2a05cfc890.jpg: 640x640 1 male, 13.6ms\n",
            "image 87/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_258_jpg.rf.d4c07741a61d3b22fd908ad9bc10fe67.jpg: 640x640 1 male, 13.6ms\n",
            "image 88/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_272_jpg.rf.ac9dfe8864ed66e93dad12dde88abf6f.jpg: 640x640 1 male, 13.6ms\n",
            "image 89/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_290_jpg.rf.e5b3f5b673df472af7e40fb111409fdf.jpg: 640x640 1 male, 13.6ms\n",
            "image 90/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_297_jpg.rf.f28db8511461cd2c25ed5bcc4908a0df.jpg: 640x640 1 male, 13.6ms\n",
            "image 91/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_301_jpg.rf.13fb20fc9036db1792483b9db0098f30.jpg: 640x640 1 female, 13.6ms\n",
            "image 92/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_31_jpg.rf.8a6c49976527e788f3201562e31ece5e.jpg: 640x640 1 male, 13.9ms\n",
            "image 93/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_33_jpg.rf.9aee64579302df769027f8343b60a473.jpg: 640x640 1 male, 13.9ms\n",
            "image 94/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_348_jpg.rf.764988f9d5ba8278b242da3ccddefb0b.jpg: 640x640 1 male, 13.9ms\n",
            "image 95/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_421_jpg.rf.8d0aeef7b483fed63592692a6c9360e6.jpg: 640x640 1 male, 13.9ms\n",
            "image 96/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_433_jpg.rf.1bc3d1a2bed690ca5c37eeaee5ccc3d1.jpg: 640x640 1 male, 13.8ms\n",
            "image 97/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_434_jpg.rf.0423d435176be2d57cecbbdb642a0e06.jpg: 640x640 1 male, 13.9ms\n",
            "image 98/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_439_jpg.rf.d448274bbf063ecfc7401fdf482a01e3.jpg: 640x640 1 male, 13.9ms\n",
            "image 99/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_450_jpg.rf.58b32c192b41ba680af7ad84d02f88af.jpg: 640x640 1 male, 13.9ms\n",
            "image 100/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_472_jpg.rf.85cf926287ccf186bae930b009cec328.jpg: 640x640 1 male, 13.9ms\n",
            "image 101/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_482_jpg.rf.1099199136ab084841f01debce75bddf.jpg: 640x640 1 male, 13.9ms\n",
            "image 102/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_484_jpg.rf.2256f41661586fd35428f05c9d123588.jpg: 640x640 1 female, 14.1ms\n",
            "image 103/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_543_jpg.rf.15331f76a25277850ccb3653dfcf0d7d.jpg: 640x640 1 female, 14.2ms\n",
            "image 104/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_547_jpg.rf.b8319afe21a9c7c57f7db4a53a8a9eea.jpg: 640x640 1 female, 14.1ms\n",
            "image 105/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_563_jpg.rf.28c8e052a97a0f1c2f33a531c5a701b8.jpg: 640x640 1 female, 14.1ms\n",
            "image 106/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_566_jpg.rf.b5a220f68053ca055307fbc05a2899e1.jpg: 640x640 1 male, 14.1ms\n",
            "image 107/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_572_jpg.rf.16df22bb268b8bbfae73cf7f9bcea116.jpg: 640x640 1 male, 14.1ms\n",
            "image 108/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_579_jpg.rf.c610700ee7eefba88c68795c3771f647.jpg: 640x640 1 female, 14.1ms\n",
            "image 109/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_580_jpg.rf.17786eb1a55b44150184f8f0cef5591c.jpg: 640x640 1 female, 14.1ms\n",
            "image 110/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_631_jpg.rf.c60eb626509632941c041b46d10aa1ca.jpg: 640x640 1 male, 14.1ms\n",
            "image 111/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_638_jpg.rf.45126e9da24e58f534753819aca70d87.jpg: 640x640 1 female, 14.1ms\n",
            "image 112/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_658_jpg.rf.5c544001c71b9fe097bbff6a833feb38.jpg: 640x640 1 male, 14.1ms\n",
            "image 113/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_700_jpg.rf.d5c09cbe531b106e1834a4e705226230.jpg: 640x640 1 male, 14.1ms\n",
            "image 114/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_720_jpg.rf.f1cec82f0ea2c064c0eb76a7bd72514c.jpg: 640x640 1 male, 14.1ms\n",
            "image 115/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_847_jpg.rf.981f157c9c0fcac7effc65afdb4a84b3.jpg: 640x640 1 male, 14.1ms\n",
            "image 116/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/face_927_jpg.rf.8ac15cfb388a6d5a65a38c3f26b1b68f.jpg: 640x640 1 female, 14.1ms\n",
            "image 117/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-354-_jpg.rf.87953bbab712edc2a7660991c3f0158a.jpg: 640x640 1 male, 14.4ms\n",
            "image 118/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-355-_jpg.rf.7cd4694c0a296a2e9f46164bd19631af.jpg: 640x640 1 male, 14.7ms\n",
            "image 119/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-37-_jpg.rf.0461c3c20bb2130f8729529d5aee6fac.jpg: 640x640 1 male, 14.4ms\n",
            "image 120/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-377-_jpg.rf.863938d758f05da6efa0ce07e789d09b.jpg: 640x640 1 male, 14.4ms\n",
            "image 121/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-378-_jpg.rf.fc812d9319de1359457a9d088770ba63.jpg: 640x640 1 male, 14.4ms\n",
            "image 122/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-382-_jpg.rf.55b7a745cea58beea2fb81564b8cd21c.jpg: 640x640 1 male, 14.4ms\n",
            "image 123/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-390-_jpg.rf.489c56ddcbf28550a56dc64fdc2d972e.jpg: 640x640 1 male, 14.3ms\n",
            "image 124/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-401-_jpg.rf.c85f25b46be05bdf7fb293a6566fab94.jpg: 640x640 1 male, 14.4ms\n",
            "image 125/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-417-_jpg.rf.35b7377fe66594c5a9b47a25f40009ed.jpg: 640x640 1 male, 14.4ms\n",
            "image 126/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-485-_jpg.rf.08ca529d14d1625a680a2e93ec3d0986.jpg: 640x640 1 male, 14.4ms\n",
            "image 127/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-496-_jpg.rf.47632eb5fe56d1f81f4df0b63ad42fc7.jpg: 640x640 1 male, 14.4ms\n",
            "image 128/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-498-_jpg.rf.e8dfd648a16851f1e2ba694fe9cebb07.jpg: 640x640 1 male, 13.8ms\n",
            "image 129/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-499-_jpg.rf.b37c78677590692c0ec8a7ed1e653704.jpg: 640x640 1 male, 13.8ms\n",
            "image 130/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-514-_jpg.rf.078f1120e429383cdc39148598231e14.jpg: 640x640 1 male, 13.8ms\n",
            "image 131/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-526-_jpg.rf.f9af518fb82395e4befb6874736159d3.jpg: 640x640 1 male, 13.9ms\n",
            "image 132/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-527-_jpg.rf.291f1ac4dca5aa3ac94d2252ce8cd4e6.jpg: 640x640 1 male, 13.9ms\n",
            "image 133/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-528-_jpg.rf.0b1e575a41e9348d67181b88b0a77dce.jpg: 640x640 1 male, 13.9ms\n",
            "image 134/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-534-_jpg.rf.4c8d988b4f3c881eacaffe4c49c2950a.jpg: 640x640 1 male, 13.9ms\n",
            "image 135/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-540-_jpg.rf.42d41c822b828f2ca50c504cf8e226bb.jpg: 640x640 1 male, 13.9ms\n",
            "image 136/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-547-_jpg.rf.a6977c0063ce46df710ad6e600059c73.jpg: 640x640 1 female, 13.9ms\n",
            "image 137/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-550-_jpg.rf.c70934978ecca5d65756dbee3732e970.jpg: 640x640 1 male, 13.9ms\n",
            "image 138/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-562-_jpg.rf.c263ddbd0247e6a814f289f5c7d49370.jpg: 640x640 1 male, 13.9ms\n",
            "image 139/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-578-_jpg.rf.13e7e22780d680903662f9464c2f6ed6.jpg: 640x640 1 male, 13.9ms\n",
            "image 140/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-580-_jpg.rf.37d773285adc0a4d266859a1a186d1f8.jpg: 640x640 1 male, 13.9ms\n",
            "image 141/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-601-_jpg.rf.a49ac0a3894241ec162b493d3f885a3f.jpg: 640x640 1 male, 13.9ms\n",
            "image 142/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-603-_jpg.rf.d3ae806d556378e68d6f0f433d589bb7.jpg: 640x640 1 male, 13.9ms\n",
            "image 143/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-610-_jpg.rf.b4655a36c9f572ed328abbcaa34362e9.jpg: 640x640 1 male, 13.9ms\n",
            "image 144/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-618-_jpg.rf.5d5e0b7d99b1d4ab9efc0755d4489e07.jpg: 640x640 1 male, 13.9ms\n",
            "image 145/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-623-_jpg.rf.e7203e611f72897e62b392f68b56dd67.jpg: 640x640 1 male, 13.8ms\n",
            "image 146/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-641-_jpg.rf.22bce6c3a95d2f0b1f3591cd913157c3.jpg: 640x640 1 male, 13.8ms\n",
            "image 147/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-644-_jpg.rf.a0ce2a9906b56b4a33d88ec33c0fecba.jpg: 640x640 1 male, 13.8ms\n",
            "image 148/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-648-_jpg.rf.2df9f135fb8ca91812e3760b90c31287.jpg: 640x640 1 male, 13.8ms\n",
            "image 149/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-65-_jpg.rf.6483cf11597621271913a123aa9f4976.jpg: 640x640 (no detections), 13.8ms\n",
            "image 150/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-650-_jpg.rf.9b07a952482cca0e29c003bd46677063.jpg: 640x640 1 male, 12.4ms\n",
            "image 151/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-74-_jpg.rf.f680ea63f962f39dff8a0aefdcaad15d.jpg: 640x640 1 female, 1 male, 12.4ms\n",
            "image 152/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/gender-75-_jpg.rf.2d813e39e78e7777c1daaf72a5f45437.jpg: 640x640 1 female, 12.4ms\n",
            "image 153/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/real_00007_jpg.rf.3babaaf76358f378db7ca8fcb88ce35f.jpg: 640x640 (no detections), 12.4ms\n",
            "image 154/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/real_00077_jpg.rf.1c615f28a2c8176e65f142a687cd1769.jpg: 640x640 1 male, 12.4ms\n",
            "image 155/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/real_00193_jpg.rf.d4ffb291d545f94c72710797fd0638ff.jpg: 640x640 1 male, 12.4ms\n",
            "image 156/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/real_01047_jpg.rf.92fef82cdc603d9b738621deaca2cb2b.jpg: 640x640 1 female, 12.4ms\n",
            "image 157/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/youtube-134_jpg.rf.420ae2255d861bdc2fc9b5d1a923eec6.jpg: 640x640 (no detections), 12.4ms\n",
            "image 158/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/youtube-180_jpg.rf.836fa8cc0d08cc05ebf8eee164752664.jpg: 640x640 1 male, 12.4ms\n",
            "image 159/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/youtube-39_jpg.rf.9f7014706f173772ca6fc385e3811149.jpg: 640x640 (no detections), 12.4ms\n",
            "image 160/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/youtube-72_jpg.rf.16af6836d4e832098e37063707e5ca9e.jpg: 640x640 1 male, 12.4ms\n",
            "image 161/161 /content/drive/MyDrive/YOLov8_dataset_gender/test/images/youtube-79_jpg.rf.f7da87eecfae18be3092a6abfcccaaae.jpg: 640x640 (no detections), 12.4ms\n",
            "Speed: 1.6ms preprocess, 13.5ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/runs/detect/predict /content/drive/MyDrive/YOLov8_dataset_gender/Result/gender"
      ],
      "metadata": {
        "id": "7eFwwx7io8dW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ujkDeT-Vqn_f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}